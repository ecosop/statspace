<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Johannes Brachem &amp; Christian Treffenstädt" />


<title>Multiple Analysen</title>

<script src="site_libs/header-attrs-2.6/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R-Space</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Methoden-Skripte
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="konfundierung.html">Konfundierung</a>
    </li>
    <li>
      <a href="kausalitaet.html">Kausalität und Korrelation</a>
    </li>
    <li>
      <a href="multilevel.html">Multilevel Modelle</a>
    </li>
    <li>
      <a href="multiple_analyses.html">Multiple Analysen und Befundmuster</a>
    </li>
    <li>
      <a href="regression_to_mean.html">Regression zur Mitte</a>
    </li>
    <li>
      <a href="effektstaerke.html">Signifikanz und Effektstärke</a>
    </li>
    <li>
      <a href="moderation_mediation.html">Moderation und Mediation</a>
    </li>
    <li>
      <a href="nhst.html">Nullhypothesentesten</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown-header">Georg-Elias-Müller-Institut für Psychologie</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Multiple Analysen</h1>
<h4 class="author">Johannes Brachem &amp; Christian Treffenstädt</h4>

</div>


<p>Paul und Marie arbeiten gemeinsam an ihrem Masterarbeitsprojekt. Sie haben insgesamt vier statistische Tests ihrer Hypothese durchgeführt, von denen zwei Tests ein signifikantes Ergebnis und zwei ein nicht signifikantes Ergebnis lieferten. Nun diskutieren Sie über die Interpretation.</p>
<p>Marie sieht die Ergebnisse als Bestätigung der Hypothese an: „Wenn es keinen Effekt gäbe, dann wäre es ziemlich unwahrscheinlich, dass von vier Tests zwei signifikant werden. Ich finde, das spricht eher dafür, dass unsere Hypothese stimmt."</p>
<p>Paul dagegen ist skeptisch: „Zwei von vier, das ist doch nur eine 50/50 Chance! Wenn unsere Hypothese stimmen würde, dann müssten das doch alle vier Tests zeigen. Ich glaube, das war reiner Zufall."</p>
<div id="wer-hat-recht" class="section level2">
<h2>Wer hat recht?</h2>
<p>Die Diskussion von Paul und Marie ist ein schöner Anlass, um über <em>Befundsmuster</em> zu sprechen, anstatt nur über einzelne Befunde.</p>
<p>Um eine fundierte Einschätzung zu erreichen, müssen wir das Problem unter verschiedenen Gesichtspunkten betrachten.</p>
<p>Wir haben zu diesem Zweck eine Simulationsstudie durchgeführt, in der wir Pauls und Maries Analyse nachempfunden haben. Das heißt, wir gehen umgekehrt vor: Wir basteln uns verschiedene Datensätze, in denen wir den <em>wahren</em> Effekt genau kennen, und schauen, welche Muster sich in der Analyse ergeben. So können wir Pauls und Maries Befundmuster besser einschätzen.</p>
<p>Wir gehen die verschiedenen Aspekte nun Schritt für Schritt durch und fassen sie am Ende noch einmal zusammen.</p>
</div>
<div id="simulationsstudie" class="section level2">
<h2>Simulationsstudie</h2>
<div id="das-modell" class="section level3">
<h3>Das Modell</h3>
<p>Zuerst müssen wir für Transparenz sorgen, damit klar ist, welche Modelle wir simulieren und berechnen. Unser Modell ist zu Veranschauungszwecken ziemlich einfach: Die vier Tests sind jeweils einfache lineare Regressionen, bei denen sowohl der Prädiktor, als auch die abhängige Variable normalverteilt und z-standardisiert sind. Das hier sind unsere zugehörigen Regressionsgleichungen:</p>
<p><span class="math display">\[\begin{array}{ll}
y_{i1} = &amp; \beta_{01} + \beta_{11} \cdot x_i + \epsilon_{i1}\\
y_{i2} = &amp; \beta_{02} + \beta_{12} \cdot x_i + \epsilon_{i2}\\
y_{i3} = &amp; \beta_{03} + \beta_{13} \cdot x_i + \epsilon_{i3}\\
y_{i4} = &amp; \beta_{04} + \beta_{14} \cdot x_i + \epsilon_{i4}\\
&amp; \text{mit} \ i=1, ..., n
\end{array}\]</span></p>
<p>Oder kompakt ausgedrückt:</p>
<p><span class="math display">\[\begin{array}{ll}
y_{ij} = &amp; \beta_{0j} + \beta_{1j} \cdot x_i + \epsilon_{ij}\\
&amp; \text{mit} \ i=1, ..., n \\
&amp; \text{und} \ j=1, ..., 4
\end{array}\]</span></p>
<p>Ein paar Anmerkungen dazu:</p>
<ul>
<li>Unsere Stichprobengröße ist <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(y_{ij}\)</span> ist der Wert einer bestimmten abhängigen Variable <span class="math inline">\(j\)</span> für eine Versuchsperson <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(x_i\)</span> ist unser Prädiktor, also unsere unabhängige Variable. Diese Variable ist für alle abhängigen Variablen gleich und hat deshalb keinen Index <span class="math inline">\(j\)</span>.</li>
<li>Der Effekt, den wir schätzen möchten, ist die Größe von <span class="math inline">\(\beta_{1j}\)</span>, also der durchschnittliche Effekt des Prädiktors <span class="math inline">\(x_i\)</span> auf die jeweilige abhängige Variable.</li>
<li>Da sowohl <span class="math inline">\(x_i\)</span>, als auch <span class="math inline">\(y_{ij}\)</span> Normalverteilt mit Mittelwert 0 und Standardabweichung 1 sind (wir haben die Daten so simuliert), ist <span class="math inline">\(\beta_{1j}\)</span> eine <em>standardisierte</em> Effektstärke mit der Einheit <em>Standardabweichungen</em> (ähnlich wie Cohen’s d).</li>
<li>Wir führen 500 Simulationen durch, das heißt wir erzeugen 500 mal Zufallsdaten und analysieren diese Zufallsdaten, damit wir uns anschauen können, wie häufig welche Ergebnisse bei unseren Analysen herauskommen.</li>
</ul>
<p>In jedem der vier Tests wird diese Hypothese getestet: <span class="math display">\[H_0: \beta_{1j} = 0 \enspace \text{vs.} \enspace H_1: \beta_{1j} \neq 0\]</span></p>
<p>Dabei nutzen wir zu Veranschauungszwecken in jeden einzelnen Test ein Alpha-Niveau von <span class="math inline">\(0.05\)</span>, d.h. wenn die Wahrscheinlichkeit für unser Ergebnis unter Annahme der <span class="math inline">\(H_0\)</span> nur eine Wahrscheinlichkeit von 5 % oder kleiner hätte, werten wir das Ergebnis als signifikant und verwerfen die Nullhypothese.</p>
</div>
<div id="was-passiert-wenn-es-tatsächlich-keinen-effekt-gibt" class="section level3">
<h3>Was passiert, wenn es tatsächlich keinen Effekt gibt?</h3>
<p>Schauen wir uns zunächst den Fall an, dass die Nullhypothese stimmt, also</p>
<p><span class="math display">\[\beta_{1j} = 0.\]</span> Die Frage, die uns interessiert ist: <strong>Wie wahrscheinlich sind folgende Szenarien?</strong></p>
<ul>
<li>0/4 Tests werden signifikant</li>
<li>1/4 Tests werden signifikant</li>
<li>2/4 Tests werden signifikant (Pauls und Maries Situation)</li>
<li>3/4 Tests werden signifikant</li>
<li>4/4 Tests werden signifikant</li>
</ul>
<p>Wir simulieren dafür unsere vier Analysen 500 mal, jeweils mit einer Stichprobengröße von <span class="math inline">\(n = 25\)</span>. Hier das Ergebnis:</p>
<p><img src="multiple_analyses_files/figure-html/unnamed-chunk-14-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Wichtige Punkte dabei:</p>
<ul>
<li><p>Wir können in diesem Plot die <strong>Alpha-Fehler-Inflation</strong> durch multiples Testen beobachten: Nur in 79 % der Fälle waren alle vier Tests nicht signifikant - in 21 % der Fälle war <em>mindestens</em> einer der Tests signifikant, <em>obwohl der wahre Effekt 0 ist</em>. Das ist der Grund, warum es wichtig ist, alle Tests, auch nicht signifikante, zu berichten. Wenn nur die signifikanten Tests berichtet werden, dann gibt es zu viele falsch-positive Befunde.</p></li>
<li><p>Dass von vier Tests zwei oder mehr signifikant werden, wenn die Nullhypothese stimmt, ist extrem unwahrscheinlich: In gerade einmal 2 % der Fälle gab es zwei signifikante Ergebnisse, und in unseren 500 Simulationen kamen drei oder vier signifikante Ergebnisse kein einziges Mal vor. Genauer:</p></li>
</ul>
</div>
<div id="was-passiert-wenn-es-einen-effekt-gibt" class="section level3">
<h3>Was passiert, wenn es einen Effekt gibt?</h3>
<p>Das hängt sehr stark davon ab, wie groß unsere <strong>Power</strong> (auch Teststärke genannt) ist. Die Power wiederum hängt davon ab, wie groß der wahre Effekt tatsächlich ist, und wie viele Datenpunkte wir für unsere Analyse zur Verfügung haben. Einen großen Effekt können wir auch schon mit wenig Datenpunkten finden, während wir für einen kleinen Effekt viele Datenpunkte brauchen.</p>
<p><strong>Power</strong> | Die Wahrscheinlichkeit, mit der wir in einem <em>einzelnen</em> Test korrekterweise die Nullhypothese zurückweisen, wenn es tatsächlich einen Effekt gibt; d.h. die Wahrscheinlichkeit für richtig-positive Befunde.</p>
<div id="verschiedene-effektstärken" class="section level4">
<h4>Verschiedene Effektstärken</h4>
<p>Bleiben wir zunächst einmal bei unserer Stichprobengröße von <span class="math inline">\(n = 25\)</span> und schauen uns an, wie viele Tests bei verschiedenen zugrundeliegenden Effektstärken signifikant werden:</p>
<p><img src="multiple_analyses_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Wichtige Punkte dabei:</p>
<ul>
<li>Das linke Panel zeigt noch einmal den Plot von oben zum Vergleich, mit einer aus der Simulation abgeleiteten Rat von falsch-positiven Befunden (ob man den Anteil signifikanter Ergebnisse “Power” oder “Falsch-Positiv-Rate” nennt, hängt davon ab, ob es einen wahren Effekt gibt).</li>
<li>Auch wenn es einen wahren Effekt gibt, kann es durchaus sein dass <strong>keiner</strong> von vier Tests zu einem signifikanten Ergebnis führt. Siehe dazu vor allem das Panel “beta = 0.2” im Plot. Dort ist es das mit 48 % <em>häufigste</em> Ergebnis, dass keiner der Tests signifikant wird. Das liegt an der sehr schwachen Power von 0.16.</li>
<li>Je größer die Effektstärke, desto häufiger kommt es selbst bei einer kleinen Stichprobe von <span class="math inline">\(n = 25\)</span> vor, dass zwei oder mehr Tests signifikant werden.</li>
</ul>
</div>
<div id="verschiedene-stichprobengrößen" class="section level4">
<h4>Verschiedene Stichprobengrößen</h4>
<p>Jetzt gehen wir einen Schritt weiter, und schauen uns das Befundmuster noch für zwei zusätzliche Stichprobengrößen an, nämlich <span class="math inline">\(n = 50\)</span> und <span class="math inline">\(n = 100\)</span>.</p>
<p><img src="multiple_analyses_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Wichtige Punkte dabei:</p>
<ul>
<li>Die linke Spalte (wahrer Effekt ist 0) zeigt sich unbeeindruckt von der Stichprobengröße: Die Alpha-Fehler Inflation bleibt konstant. (Aber Vorsicht! Sie ist nicht grundsätzlich konstant, sondern nur unabhängig von der Stichprobengröße. Doch je mehr Tests durchgeführt werden, desto stärker wird die Inflation.)</li>
<li>Mit größerer Stichprobe verschiebt sich das Befundmuster für alle Fälle, in denen es einen wahren Effekt gibt, nach rechts: Ein immer größerer Anteil der vier Tests “schlägt an”. Das liegt an der mit der Stichprobengröße steigenden Power.</li>
<li>Auch wenn es einen wahren Effekt gibt und wir eine hohe Stichprobengröße gibt, gibt es längst keine Garantie dafür, dass jeder Test signifikant wird. Ein gutes Beispiel dafür ist die Spalte mit “beta = 0.2”. Hier werden selbst bei einer Stichprobengröße von <span class="math inline">\(n = 100\)</span> in cs. 69 % der Fälle nur zwei oder weniger Tests signifikant.</li>
</ul>
</div>
</div>
<div id="welchen-einfluss-hat-die-anzahl-von-tests" class="section level3">
<h3>Welchen Einfluss hat die Anzahl von Tests?</h3>
<p>Bisher haben wir die Anzahl von Tests konstant bei vier gehalten, damit unsere Simulation mit der Analyse von Paul und Marie vergleichbar bleibt. Doch was passiert, wenn wir nicht vier, sondern zehn Tests durchführen? Das können wir im nächsten Plot sehen. Wir beschränken uns dafür wieder auf eine Stichprobengröße von <span class="math inline">\(n = 25\)</span>.</p>
<p><img src="multiple_analyses_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Wichtige Punkte dabei:</p>
<ul>
<li><p>Die <strong>Alpha-Fehler-Inflation</strong> steigt mit steigender Testzahl. Mit 36 %-prozentiger Wahrscheinlichkeit ist hier <em>mindestens</em> ein Test signifikant, obwohl die wahre Effektstärke 0 ist. Wir können also wirklich nicht einfach viele Tests machen und “schauen, was funktioniert” - das führt uns in die Irre. Wir müssen das gesamt Muster betrachten.</p></li>
<li><p>Geringe Power führt dazu, dass trotz eines wahren Effekts &gt; 0 nur ein Bruchteil unserer Tests signifikant wird. Umgekehrt bedeutet das: Wenn in einer Studie viele Tests für eine Hypothese berichtet werden, und trotz kleiner oder moderater Power <em>alle</em> oder <em>fast alle</em> Tests signifikant werden, ist das ein Grund, misstrauisch zu sein: Ein solches Muster ist bei vollständiger Berichterstattung extram unwahrscheinlich!</p></li>
</ul>
</div>
</div>
<div id="auf-den-punkt-gebracht" class="section level2">
<h2>Auf den Punkt gebracht</h2>
<p>Ganz knapp zusammengefasst: <strong>Marie liegt richtig</strong>. Wenn es keinen wahren Effekt gäbe, dann wäre es sehr unwahrscheinlich, dass zwei von vier Tests signifikante Ergebnisse liefern: Unsere Simulation ergab dafür nur eine Wahrscheinlichkeit von ca. 2 % für dieses Muster unter der Annahme von <span class="math inline">\(\beta=0\)</span>. Selbst im Minimal-Szenario mit kleinem Effekt und kleiner Stichprobengröße (<span class="math inline">\(\beta=0.2, n = 25\)</span>) war die Wahrscheinlichkeit mit 11 % um das 5.5-fache höher!</p>
<div class="alert alert-success">
<p><strong>Wir wissen jetzt genauer, worauf es dabei im Einzelnen ankommt</strong>:</p>
<p>Multiples Testen führt zu <em>Alpha-Fehler-Inflation</em>. Die ist problematisch, wenn mehrere Tests durchgeführt, aber nur signifikante Tests berichtet werden.</p>
<p>Das genaue Befundmuster bei multiplem Testen hängt davon ab …</p>
<ul>
<li>… wie groß der wahre Effekt ist (-&gt; hängt mit Power zusammen, kann aber nicht von Forschenden beeinflusst werden.)</li>
<li>… wie groß die Stichprobe ist (-&gt; an diesem Punkt können Forschende ihre Power beeinflussen!)</li>
<li>… wie viele Tests durchgeführt werden</li>
</ul>
</div>
<div id="aus-verschiedenen-perspektiven" class="section level3">
<h3>Aus verschiedenen Perspektiven</h3>
<p>Die Informationen aus diesem Skript helfen uns, besser einzuschätzen, ob ein bestimmtes Befundmuster <em>für</em> oder <em>gegen</em> eine Hypothese spricht. Was das für Forschende und Praktiker für eine Bedeutung hat, haben wir in unserem Skript zu <a href="https://ctreffe.github.io/r-space/konfundierung.html">Konfundierung</a> ausführlich dargelegt.</p>
</div>
</div>
<div id="zusatzinfos" class="section level2">
<h2>Zusatzinfos</h2>
<p>Wir haben bis hierher die wichtigsten Informationen vermittelt. Die Zusatzinfo-Kästen ab hier bieten die Möglichkeit zur Vertiefung.</p>
<div class="alert alert-info">
<p><strong>Zusatzinfo 1: Befundmuster und Meta-Analysen</strong></p>
<p>Wir haben hier Befundmuster aus der Perspektive betrachtet, dass <em>mehrere, einzelne</em> Tests in <em>einer Studie</em> durchgeführt wurden und nebeneinander stehen. Ein anderer häufiger Fall, in dem die Interpretation von Befundmustern wichtig ist, ist die Interpretation der Ergebnisse von mehreren Studien. Dafür sind die Erkentnisse aus diesem Skript ebenfalls durchaus hilfreich!</p>
<p>Doch wenn man wirklich ins Detail einsteigen möchte, dann bietet es sich an, mehrere Befunde in Form einer Meta-Analyse miteinander zu verrechnen. So können verschiedene Tests zusammengeführt und in einer einzigen Schätzung konzentriert werden. Im Ergebnis kann man also aus einem Befundmuster wieder einen einzigen Test errechnen.</p>
<p>Meta-Analytisches Vorgehen ist vor allem üblich zur Zusammenführung der Ergebnisse aus mehreren verschiedenen Forschungsarbeiten, kann aber durchaus auch zur Zusammenführung der Ergebnisse mehrerer Studien in einem einzigen Paper verwendet werden.</p>
</div>
<div class="alert alert-info">
<p><strong>Zusatzinfo 2: Simulationen und analytisches Vorgehen</strong></p>
<p>Wir haben hier zu Veranschaulichung die Ergebnisse von Simulationsstudien vorgestellt, doch es ist durchaus möglich, die von uns hier vorgestellten Befunde rein mathematisch auszurechnen.</p>
<p>Die Befundmuster unter der Nullhypothese folgen bspw. einer Binomialverteilung. Wir können die Wahrscheinlichkeit der verschiedenen Möglichkeiten mit der Formel der Binomialverteilung ausrechnen:</p>
<p><span class="math display">\[P(k|n,p) = \binom{n}{k} p^k (1-p)^{n-k}\]</span> Dabei ist</p>
<ul>
<li><span class="math inline">\(n\)</span> die Anzahl der Versuche (in unserem Fall die Gesamtzahl von Tests)</li>
<li><span class="math inline">\(k\)</span> die Anzahl der “Erfolge” (in unserem Fall die Zahl signifikanter Ergebnisse)</li>
<li><span class="math inline">\(p\)</span> die Wahrscheinlichkeit eines “Erfolgs” (in unserem Fall das Alpha-Niveau, wenn es keinen wahren Effekt gibt, bzw. die Power, wenn es einen gibt)</li>
</ul>
<p><br />
Rechnen wir das einmal aus, für den Fall unseres Alpha-Niveaus von <span class="math inline">\(\alpha = 0.05\)</span> und <span class="math inline">\(\beta_{1j} = 0\)</span>. Wie wahrscheinlich ist es, dass <span class="math inline">\(k = 1\)</span> von <span class="math inline">\(n = 4\)</span> Tests erfolgreich sind?</p>
<p><span class="math display">\[\begin{array}{ll}
P(k = 1|n=4,p=0.05) &amp; = \binom{4}{1}0.05^1(1-0.05)^{4-1}\\
 &amp; = 4 \cdot 0.05 \cdot 0.95^3\\
 &amp; \approx 0.17
\end{array}\]</span></p>
<p>Damit sind wir mit unserem simulierten Ergebnis von <span class="math inline">\(P(k=1)=0.19\)</span> schon recht nah gekommen. Die noch recht große Abweichung von <span class="math inline">\(0.02\)</span> ergibt aus unserer Rundung auf zwei Nachkommastellen und dem für eine Simulationsstudie noch recht kleinen Umfang von 500 Simulationen.</p>
<p>Wenn wir die gesamte Alpha-Fehler-Inflation berechnen wollen, rechnen wir <span class="math display">\[\begin{array}{ll}
P(k\neq0|n=4,p=0.05) &amp; = P(k=1|n,p) + P(k=2|n,p) + P(k3|n,p) + P(k=4|n,p)\\
 &amp; = 0.171475 + 0.0135375 + 0.000475 + 0.000000625\\
 &amp; \approx 0.185 
\end{array}\]</span></p>
<p>Auch <strong>Power-Berechnungen</strong> können analytisch durchgeführt werden, aber das übersteigt nun doch den Horizont dieses Zusatzinfo-Kastens.</p>
</div>
<div class="alert alert-info">
<p><strong>Zusatzinfo 3: Multiples Testen und Testplanung</strong></p>
<p>Dieses Skript wurde geschrieben, um Situationen abzudecken, in denen man mit multiplen Analysen konfrontiert ist, und das beste tun möchte, eine angemessene Interpretation zu finden. Daraus leitet sich nicht direkt ab, dass Experimente mit solchen multiplen Analysen geplant werden sollten. In der Planung von Studien macht es im Gegenteil häufig eher Sinn, wann immer möglich weniger Studien (und dadurch weniger Tests) mit größeren Stichproben durchzuführen, als mehr Studien mit kleineren Stichproben.</p>
</div>
<div class="alert alert-info">
<p><strong>Zusatzinfo 4: Alternative Interpretationsmöglichkeiten</strong></p>
<p>Eine sehr beliebte Alternative (oder Ergänzung) zu dem hier vorgestellten Vorgehen bei der Interpretation multipler Tests ist die Korrektur des Alpha-Niveaus, bspw. die Bonferroni-Korrektur. Durch diese Korrektur kann erreicht werden, dass die <em>gesamte</em> falsch-positiv-Rate nicht größer als das Alpha-Niveau wird.</p>
<p>Bei der Bonferroni-Korrektur wird das Alpha-Niveau durch die Anzahl der Tests geteilt. Die Nullhypothese wird dann verworfen, wenn in <em>einem</em> der Tests ein signifikantes Ergebnis vorliegt. Schauen wir uns das am besten mit einem Beispiel an. Nehmen wir einmal an, wir wollen testen, ob <a href="https://xkcd.com/882/">“Jelly Beans” Akne hervorrufen</a>. Es gibt 20 verschiedene Farben von Jelly Beans. Wir führen für jede Farbe einen Test durch. Legen wir für jeden Test ein Alpha-Niveau von 0.05 zugrunde, dann ist es sehr wahrscheinlich, dass einer oder mehr unserer Tests signifikant wird. Genau genommen beträgt die Wahrscheinlichkeit, dass einer oder mehr Tests falsch-positiv ein signifikantes Ergebnis zeigen gerundet ca. 64 %. Wir können das mit der Binomialverteilung genau berechnen (siehe Zusatzinfo 2):</p>
<span class="math display">\[\begin{array}{ll}
P(k \neq 0|n=20, p=0.05) &amp; = 1 - P(k = 0|n=20, p=0.05) \\
&amp; = 1 - \binom{20}{0}0.05^0(1-0.05)^{20-0} \\
&amp; = 1 - 1\cdot 1 \cdot 0.95^{20} \\
&amp; = 1 - 0.95^{20} \\
&amp; = 1 - 0.3584859 \\
&amp; \approx 0.641
\end{array}\]</span>
<p>Das heißt, praktisch haben wir über alle Tests zusammengerechnet ein Alpha-Niveau von <span class="math inline">\(\alpha_{family} \approx 0.64\)</span>. Das nennen wir auch das <em>family-wise</em> Alpha-Niveau, weil es sich auf eine <em>Familie</em> von zusammengehörenden Tests bezieht. Nun korrigieren wir das Alpha-Niveau mit der Bonferroni-Korrektur:</p>
<span class="math display">\[\begin{array}{ll}

\alpha_{bf} &amp; = \frac{\alpha}{n} \\
&amp; = 0.05 / 20 \\
&amp; = 0.0025
\end{array}\]</span>
<p>Schauen wir nun, welche falsch-positiv-Rate wir mit <span class="math inline">\(\alpha_{bf} = 0.0025\)</span> erhalten:</p>
<span class="math display">\[\begin{array}{ll}
P(k \neq 0|n=20, p=0.0025) &amp; = 1 - P(k = 0|n=20, p=0.0025) \\
&amp; = 1 - \binom{20}{0}0.0025^0(1-0.0025)^{20-0} \\
&amp; = 1 - 1\cdot 1 \cdot 0.9975^{20} \\
&amp; = 1 - 0.9975^{20} \\
&amp; = 1 - 0.9511699 \\
&amp; \approx 0.049
\end{array}\]</span>
<p>Wir konnten unser <em>family-wise</em> Alpha-Niveau also erfolgreich auf <span class="math inline">\(0.049\)</span> korrigieren. Eine Sache kann jedoch auffallen: Unser family-wise Alpha-Niveau liegt nun sogar <em>unter</em> 0.05! Aus diesem Grund gilt die Bonferroni-Korrektur als eine konservative Korrektur: Sie drückt die falsch-positiv-Rate <em>etwas</em> stärker als es das ursprüngliche Alpha-Niveau vorgeben würde.</p>
<p><strong>Welche Variante zur Analyse multiple Befunde sollte man nun wählen?</strong></p>
<p><em>Das kommt darauf an</em>. Die Korrektur des Alpha-Niveaus, insbesondere die Bonferroni- Korrektur ist einfach und schnell erledigt. Die oben im Skipt vorgestellte Analyse ist aufwendiger, aber genauer. Es kommt also wie so häufig darauf an, welche Kriterien bei der Analyse im Vordergrund stehen. Soll es schnell gehen, dann ist eine Bonferroni-Korrektur vertretbar. Andernfalls ist es hilfreich, sich die Ergebnisse detailliert anzusehen.</p>
</div>
</div>
<div id="daten-und-skript" class="section level1">
<h1>Daten und Skript</h1>
<p>Hier können die Daten und das Skript der Datensimulation heruntergeladen werden:</p>
<ul>
<li><a href="https://raw.githubusercontent.com/ctreffe/r-space/master/files/sim_4avs.csv">sim_4avs.csv (4 Tests)</a></li>
<li><a href="https://raw.githubusercontent.com/ctreffe/r-space/master/files/sim_10avs.csv">sim_10avs.csv (10 Tests)</a></li>
<li><a href="https://raw.githubusercontent.com/ctreffe/r-space/master/multiple_analyses.Rmd">multiple_analyses.Rmd (Skript)</a></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
